{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acac253d-c84c-4ed1-905f-3184f7d557f3",
   "metadata": {},
   "source": [
    "# Scrape stat data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b8dcb8-4a60-4bc4-a6c4-dd22d58eece2",
   "metadata": {},
   "source": [
    "References:\n",
    "* Main source of scraped statistics: http://japan-mk.blog.jp/\n",
    "* Additional source of statistics: https://www.mariowiki.com/Mario_Kart_8_Deluxe_in-game_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadf67e6-bb7f-44f7-9db7-49b9211c4a05",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b331c53f-c851-42af-acf6-fc5559769746",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eb81662-f9cb-4add-a9a7-5ae343b46c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Async-related imports\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "# Web scraping imports\n",
    "import urllib.request\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Data processing imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d799545-9e43-40d4-a8e7-032d4812cc0c",
   "metadata": {},
   "source": [
    "### Pre-initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97dd9e73-bd0c-4f25-95d2-0a4e7a42955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def read_json(filename):\n",
    "    with open(filename) as fp:\n",
    "        return json.load(fp)\n",
    "    \n",
    "# Pre-initialization\n",
    "IS_RUNNING_NOTEBOOK = '__file__' not in globals()\n",
    "IS_RUNNING_BOKEH = __name__.startswith('bokeh')\n",
    "\n",
    "MAIN_DIRECTORY = os.path.abspath('') if IS_RUNNING_NOTEBOOK else os.path.dirname(__file__)\n",
    "ROOT_DIRECTORY = str(Path(MAIN_DIRECTORY).parent.absolute())\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load Japanese-to-English translations\n",
    "JA_EN_TRANSLATE_TECHNICAL  = read_json(ROOT_DIRECTORY + '/translations/ja_en/technical.json')\n",
    "JA_EN_TRANSLATE_CHARACTERS = read_json(ROOT_DIRECTORY + '/translations/ja_en/characters.json')\n",
    "JA_EN_TRANSLATE_FRAMES     = read_json(ROOT_DIRECTORY + '/translations/ja_en/frames.json')\n",
    "JA_EN_TRANSLATE_TIRES      = read_json(ROOT_DIRECTORY + '/translations/ja_en/tires.json')\n",
    "JA_EN_TRANSLATE_GLIDERS    = read_json(ROOT_DIRECTORY + '/translations/ja_en/gliders.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7659ff98-b4c9-4b3f-a129-cf93958d63ce",
   "metadata": {},
   "source": [
    "## Run main logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1a6479-7795-4aaa-b36a-829bd58731d5",
   "metadata": {},
   "source": [
    "### Define low-level logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e49f67d4-5649-4ea5-a801-0e4d0337a5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_page_sources(urls, stat_tab=3):\n",
    "    page_sources = []\n",
    "    \n",
    "    async with async_playwright() as p:\n",
    "        # Open a new browser\n",
    "        browser = await p.chromium.launch()\n",
    "        \n",
    "        # Open each URL in a new tab to conserve resources\n",
    "        for (i, url) in enumerate(urls):\n",
    "            page = await browser.new_page()\n",
    "            await page.goto(url)\n",
    "\n",
    "            # Change stat tab\n",
    "            href = await page.evaluate(f'() => $(\".sw_sec{stat_tab}\").click()')\n",
    "\n",
    "            # Grab HTML content\n",
    "            page_sources.append(await page.content())\n",
    "            \n",
    "        # Close the browser session once we're done\n",
    "        await browser.close()\n",
    "    \n",
    "    return page_sources\n",
    "\n",
    "\n",
    "def extract_table_data_from_src(html_doc, translation_table):\n",
    "    html_soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    main_table = html_soup.select_one('table')\n",
    "\n",
    "    # Image tags don't have a proper text string to help identify each row, so we're extracting it from the 'alt' attribute\n",
    "    for img_tag in main_table.select('tbody > tr > td > img'):\n",
    "        img_tag.replace_with(img_tag['alt'], ' / ')\n",
    "\n",
    "    # Replace Japanese text with English\n",
    "    translated_main_table = str(main_table)\n",
    "    for (ja_text, en_text) in translation_table.items():\n",
    "        translated_main_table = translated_main_table.replace(ja_text, en_text)\n",
    "\n",
    "    # Use Pandas to parse the table and standardize the dataframe\n",
    "    table_df = pd.read_html(translated_main_table, flavor='html5lib')[0]\n",
    "    table_df = standardize_dataframe(table_df)\n",
    "    return table_df\n",
    "\n",
    "\n",
    "def standardize_dataframe(df):\n",
    "    def _to_numeric(cell):\n",
    "        try:\n",
    "            return float(cell)\n",
    "        except ValueError:\n",
    "            return cell\n",
    "    \n",
    "    def _remove_dupes(arr):\n",
    "        return list(dict.fromkeys(arr))\n",
    "    \n",
    "    # Convert all cells to numeric if possible\n",
    "    df = df.applymap(_to_numeric)\n",
    "    \n",
    "    # Merge the multi-index to a single index\n",
    "    df = df.set_axis(\n",
    "        np.apply_along_axis(\n",
    "            # HACK: We need to wrap the row in an numpy array since a string is an\n",
    "            # array of characters and the output of the function expects a 1D array of sorts\n",
    "            # Additionally, not casting it as an 'object' will trim the string contents to the shortest string size\n",
    "            lambda row: np.array(\" - \".join( _remove_dupes(row) ), dtype='object'),\n",
    "            axis=1,\n",
    "            arr=np.array(df.columns.tolist(), dtype = 'object')\n",
    "        ),\n",
    "        axis='columns'\n",
    "    )\n",
    "    \n",
    "    # Remove rows that have all strings. This works by collecting all present types across each row, getting its\n",
    "    # string representation, removing the 'str' type and checking if any other types remain\n",
    "    df = df[\n",
    "        np.apply_along_axis(\n",
    "            func1d=lambda row: len(set([type(cell).__name__ for cell in row]) - {'str'}),\n",
    "            axis=1,\n",
    "            arr=df.to_numpy()\n",
    "        ) == 1\n",
    "    ]\n",
    "    \n",
    "    # Remove the ending slashes from the first column\n",
    "    df.iloc[:, 0] = df.iloc[:, 0].str.rstrip(to_strip=' /')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5d50da-66f5-407e-9a1c-fcfc310d8656",
   "metadata": {},
   "source": [
    "### Scrape stats from japan-mk.blog.jp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce52e88a-f18c-48de-bdcc-ffd5e40d41f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching all page statistics sources...done!\n"
     ]
    }
   ],
   "source": [
    "async def scrape_sources():\n",
    "    return await fetch_page_sources([\n",
    "        'http://japan-mk.blog.jp/mk8dx.st-c', # Characters\n",
    "        'http://japan-mk.blog.jp/mk8dx.st-f', # Frames\n",
    "        'http://japan-mk.blog.jp/mk8dx.st-t', # Tires\n",
    "        'http://japan-mk.blog.jp/mk8dx.st-g', # Gliders\n",
    "    ], stat_tab=1)\n",
    "\n",
    "MAIN_LOOP = asyncio.get_event_loop()\n",
    "\n",
    "print('Fetching all page statistics sources...', end='')\n",
    "\n",
    "(\n",
    "    char_stat_src,\n",
    "    frame_stat_src,\n",
    "    tire_stat_src,\n",
    "    glider_stat_src,\n",
    ") = MAIN_LOOP.run_until_complete(scrape_sources())\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f9c491-954b-4ac6-a893-0553f12e5b23",
   "metadata": {},
   "source": [
    "### Extract data from sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9f8af42-cc35-4d8b-a91b-3f50805d871a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tabular data from sources...done!\n"
     ]
    }
   ],
   "source": [
    "print('Extracting tabular data from sources...', end='')\n",
    "\n",
    "character_stats_df = extract_table_data_from_src(char_stat_src,   JA_EN_TRANSLATE_TECHNICAL | JA_EN_TRANSLATE_CHARACTERS)\n",
    "frame_stats_df     = extract_table_data_from_src(frame_stat_src,  JA_EN_TRANSLATE_TECHNICAL | JA_EN_TRANSLATE_FRAMES)\n",
    "tire_stats_df      = extract_table_data_from_src(tire_stat_src,   JA_EN_TRANSLATE_TECHNICAL | JA_EN_TRANSLATE_TIRES)\n",
    "glider_stats_df    = extract_table_data_from_src(glider_stat_src, JA_EN_TRANSLATE_TECHNICAL | JA_EN_TRANSLATE_GLIDERS)\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f351ce1-3d5c-4d79-8613-9cd73053b63d",
   "metadata": {},
   "source": [
    "### Save extracted data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a30fa92c-14df-431f-a2e0-dd4943adf50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving extracted data to CSVs..."
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ROOT_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m Path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../stats\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (name, df) \u001b[38;5;129;01min\u001b[39;00m dframes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 12\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mROOT_DIR\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/stats/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdone!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ROOT_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "print('Saving extracted data to CSVs...', end='')\n",
    "\n",
    "dframes = {\n",
    "    'Character': character_stats_df,\n",
    "    'Frame': frame_stats_df,\n",
    "    'Tire': tire_stats_df,\n",
    "    'Glider': glider_stats_df,\n",
    "}\n",
    "\n",
    "Path('../stats').mkdir(parents=True, exist_ok=True)\n",
    "for (name, df) in dframes.items():\n",
    "    df.to_csv(f'{ROOT_DIR}/stats/{name}.csv', index=False)\n",
    "    \n",
    "print('done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
